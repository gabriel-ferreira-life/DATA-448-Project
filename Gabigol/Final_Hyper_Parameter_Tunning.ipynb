{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier with the top 6 featues\n",
      "Model:  RandomForestClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10)\n",
      "Score:  0.825\n",
      " \n",
      " \n",
      "Random Forest Classifier with the top 7 featues\n",
      "Model:  RandomForestClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10)\n",
      "Score:  0.825\n",
      " \n",
      " \n",
      "Random Forest Classifier with the top 8 featues\n",
      "Model:  RandomForestClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10)\n",
      "Score:  0.825\n",
      " \n",
      " \n",
      "AdaBoost Classifier with the top 6 features\n",
      "Model:  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
      "                                                         min_samples_leaf=5,\n",
      "                                                         min_samples_split=15),\n",
      "                   learning_rate=0.001, n_estimators=300)\n",
      "Score:  0.811\n",
      " \n",
      " \n",
      "AdaBoost Classifier with the top 7 features\n",
      "Model:  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
      "                                                         min_samples_leaf=5,\n",
      "                                                         min_samples_split=15),\n",
      "                   learning_rate=0.001, n_estimators=100)\n",
      "Score:  0.786\n",
      " \n",
      " \n",
      "AdaBoost Classifier with the top 8 features\n",
      "Model:  AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3,\n",
      "                                                         min_samples_leaf=5,\n",
      "                                                         min_samples_split=15),\n",
      "                   learning_rate=0.001, n_estimators=100)\n",
      "Score:  0.813\n",
      " \n",
      " \n",
      "Decision Tree Classifier with the top 6 features\n",
      "Model:  DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=10)\n",
      "Score:  0.862\n",
      " \n",
      " \n",
      "Decision Tree Classifier with the top 7 features\n",
      "Model:  DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=15)\n",
      "Score:  0.813\n",
      " \n",
      " \n",
      "Decision Tree Classifier with the top 8 features\n",
      "Model:  DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=15)\n",
      "Score:  0.813\n",
      " \n",
      " \n",
      "Gradient Boosting Classifier with the top 6 features\n",
      "Model:  GradientBoostingClassifier(learning_rate=0.01, min_samples_leaf=5,\n",
      "                           min_samples_split=10)\n",
      "Score:  0.825\n",
      " \n",
      " \n",
      "Gradient Boosting Classifier with the top 7 features\n",
      "Model:  GradientBoostingClassifier(learning_rate=0.01, min_samples_leaf=5,\n",
      "                           min_samples_split=15)\n",
      "Score:  0.825\n",
      " \n",
      " \n",
      "Gradient Boosting Classifier with the top 8 features\n",
      "Model:  GradientBoostingClassifier(learning_rate=0.01, min_samples_leaf=5,\n",
      "                           min_samples_split=15)\n",
      "Score:  0.825\n",
      " \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Variables</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(max_depth=3, max_featu...</td>\n",
       "      <td>top 6</td>\n",
       "      <td>0.825261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(DecisionTreeClassifier(max_depth=3, max_featu...</td>\n",
       "      <td>top 7</td>\n",
       "      <td>0.825261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(DecisionTreeClassifier(max_depth=3, max_featu...</td>\n",
       "      <td>top 8</td>\n",
       "      <td>0.825261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeClassifier(max_depth=3, min_sampl...</td>\n",
       "      <td>top 6</td>\n",
       "      <td>0.811016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(DecisionTreeClassifier(max_depth=3, min_sampl...</td>\n",
       "      <td>top 7</td>\n",
       "      <td>0.786325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(DecisionTreeClassifier(max_depth=3, min_sampl...</td>\n",
       "      <td>top 8</td>\n",
       "      <td>0.812915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=3, min_sample...</td>\n",
       "      <td>top 6</td>\n",
       "      <td>0.862298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=3, min_sample...</td>\n",
       "      <td>top 7</td>\n",
       "      <td>0.812915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DecisionTreeClassifier(max_depth=3, min_sample...</td>\n",
       "      <td>top 8</td>\n",
       "      <td>0.812915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>top 6</td>\n",
       "      <td>0.825261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>top 7</td>\n",
       "      <td>0.825261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>top 8</td>\n",
       "      <td>0.825261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Models Variables    Scores\n",
       "0   (DecisionTreeClassifier(max_depth=3, max_featu...     top 6  0.825261\n",
       "1   (DecisionTreeClassifier(max_depth=3, max_featu...     top 7  0.825261\n",
       "2   (DecisionTreeClassifier(max_depth=3, max_featu...     top 8  0.825261\n",
       "3   (DecisionTreeClassifier(max_depth=3, min_sampl...     top 6  0.811016\n",
       "4   (DecisionTreeClassifier(max_depth=3, min_sampl...     top 7  0.786325\n",
       "5   (DecisionTreeClassifier(max_depth=3, min_sampl...     top 8  0.812915\n",
       "6   DecisionTreeClassifier(max_depth=3, min_sample...     top 6  0.862298\n",
       "7   DecisionTreeClassifier(max_depth=3, min_sample...     top 7  0.812915\n",
       "8   DecisionTreeClassifier(max_depth=3, min_sample...     top 8  0.812915\n",
       "9   ([DecisionTreeRegressor(criterion='friedman_ms...     top 6  0.825261\n",
       "10  ([DecisionTreeRegressor(criterion='friedman_ms...     top 7  0.825261\n",
       "11  ([DecisionTreeRegressor(criterion='friedman_ms...     top 8  0.825261"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "## Reading the data set\n",
    "train = pd.read_csv('/Users/gabrielvictorgomesferreira/Desktop/Diabetes_Project/Data/Final_Var_Eng_train.csv')\n",
    "\n",
    "\n",
    "# Defining input and target variables\n",
    "X = train.drop(['Diabetes_012'], axis = 1)\n",
    "Y = train['Diabetes_012']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, stratify = Y)\n",
    "\n",
    "# Defining top 8, 7, and 6 variables\n",
    "# Train dataset Random Forest Classifier\n",
    "RF_X_train_8 = X_train[['Interaction_1', 'Log_BMI', 'PhysHlth', 'MentHlth', 'Fruits', 'Age', 'Smoker', 'College_1_3']]\n",
    "RF_X_train_7 = X_train[['Interaction_1', 'Log_BMI', 'PhysHlth', 'MentHlth', 'Fruits', 'Age', 'Smoker']]\n",
    "RF_X_train_6 = X_train[['Interaction_1', 'Log_BMI', 'PhysHlth', 'MentHlth', 'Fruits', 'Age']]\n",
    "\n",
    "\n",
    "\n",
    "## Defining the hyper-parameters for Random Forest Classifier\n",
    "RF_param_grid = {'n_estimators': [100, 300, 500],\n",
    "                 'min_samples_split': [10, 15], \n",
    "                 'min_samples_leaf': [5, 7], \n",
    "                 'max_depth' : [3, 5, 7]}\n",
    "\n",
    "# Performing GridSearch\n",
    "RF_grid_search_6 = GridSearchCV(RandomForestClassifier(), RF_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(RF_X_train_6, Y_train)\n",
    "RF_grid_search_7 = GridSearchCV(RandomForestClassifier(), RF_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(RF_X_train_7, Y_train)\n",
    "RF_grid_search_8 = GridSearchCV(RandomForestClassifier(), RF_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(RF_X_train_8, Y_train)\n",
    "\n",
    "\n",
    "# Extracting the best model\n",
    "RF_model_6 = RF_grid_search_6.best_estimator_\n",
    "RF_score_6 = RF_grid_search_6.cv_results_\n",
    "RF_score_6 = RF_score_6['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Random Forest Classifier with the top 6 featues\")\n",
    "print(\"Model: \", RF_model_6)\n",
    "print(\"Score: \", round(RF_score_6, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Extracting the best model\n",
    "RF_model_7 = RF_grid_search_7.best_estimator_\n",
    "RF_score_7 = RF_grid_search_7.cv_results_\n",
    "RF_score_7 = RF_score_7['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Random Forest Classifier with the top 7 featues\")\n",
    "print(\"Model: \", RF_model_7)      \n",
    "print(\"Score: \", round(RF_score_7, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Extracting the best model\n",
    "RF_model_8 = RF_grid_search_8.best_estimator_\n",
    "RF_score_8 = RF_grid_search_8.cv_results_\n",
    "RF_score_8 = RF_score_8['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Random Forest Classifier with the top 8 featues\")\n",
    "print(\"Model: \", RF_model_8)\n",
    "print(\"Score: \", round(RF_score_8, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "## Defining the hyper-parameters for Ada\n",
    "# Train dataset AdaBoost with Decision Tree Classifier\n",
    "Ada_X_train_8 = X_train[['Interaction_1', 'Log_BMI', 'Veggies', 'Stroke', 'Smoker', 'Age', 'PhysHlth', 'Interaction_5']]\n",
    "Ada_X_train_7 = X_train[['Interaction_1', 'Log_BMI', 'Veggies', 'Stroke', 'Smoker', 'Age', 'PhysHlth']]\n",
    "Ada_X_train_6 = X_train[['Interaction_1', 'Log_BMI', 'Veggies', 'Stroke', 'Smoker', 'Age']]\n",
    "\n",
    "\n",
    "Ada_param_grid = {'n_estimators': [100, 300, 500],\n",
    "                 'base_estimator__min_samples_split': [10, 15], \n",
    "                 'base_estimator__min_samples_leaf': [5, 7], \n",
    "                 'base_estimator__max_depth' : [3, 5, 7],\n",
    "                 'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "## Running grid search with 3 fold\n",
    "Ada_grid_search_6 = GridSearchCV(AdaBoostClassifier(base_estimator = DecisionTreeClassifier()), Ada_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(Ada_X_train_6, Y_train)\n",
    "Ada_grid_search_7 = GridSearchCV(AdaBoostClassifier(base_estimator = DecisionTreeClassifier()), Ada_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(Ada_X_train_7, Y_train)\n",
    "Ada_grid_search_8 = GridSearchCV(AdaBoostClassifier(base_estimator = DecisionTreeClassifier()), Ada_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(Ada_X_train_8, Y_train)\n",
    "\n",
    "# Extracting the best model\n",
    "Ada_model_6 = Ada_grid_search_6.best_estimator_\n",
    "Ada_score_6 = Ada_grid_search_6.cv_results_\n",
    "Ada_score_6 = Ada_score_6['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"AdaBoost Classifier with the top 6 features\")\n",
    "print(\"Model: \", Ada_model_6)\n",
    "print(\"Score: \", round(Ada_score_6, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Extracting the best model\n",
    "Ada_model_7 = Ada_grid_search_7.best_estimator_\n",
    "Ada_score_7 = Ada_grid_search_7.cv_results_\n",
    "Ada_score_7 = Ada_score_7['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"AdaBoost Classifier with the top 7 features\")\n",
    "print(\"Model: \", Ada_model_7)\n",
    "print(\"Score: \", round(Ada_score_7, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Extracting the best model\n",
    "Ada_model_8 = Ada_grid_search_8.best_estimator_\n",
    "Ada_score_8 = Ada_grid_search_8.cv_results_\n",
    "Ada_score_8 = Ada_score_8['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"AdaBoost Classifier with the top 8 features\")\n",
    "print(\"Model: \", Ada_model_8)\n",
    "print(\"Score: \", round(Ada_score_8, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Defining top 8, 7, and 6 variables\n",
    "# Train dataset Decision Tree Classifier\n",
    "Tree_X_train_8 = X_train[['Interaction_1', 'Log_BMI', 'Veggies', 'Smoker', 'PhysActivity', 'MentHlth', 'Interaction_5', 'HeartDiseaseorAttack']]\n",
    "Tree_X_train_7 = X_train[['Interaction_1', 'Log_BMI', 'Veggies', 'Smoker', 'PhysActivity', 'MentHlth', 'Interaction_5']]\n",
    "Tree_X_train_6 = X_train[['Interaction_1', 'Log_BMI', 'Veggies', 'Smoker', 'PhysActivity', 'MentHlth']]\n",
    "\n",
    "\n",
    "\n",
    "## Defining the hyper-parameters for Decision Tree Classifier\n",
    "tree_param_grid = {'min_samples_split': [10, 15], \n",
    "                 'min_samples_leaf': [5, 7], \n",
    "                 'max_depth' : [3, 5, 7]}\n",
    "\n",
    "# Performing GridSearch\n",
    "tree_grid_search_6 = GridSearchCV(DecisionTreeClassifier(), tree_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(Tree_X_train_6, Y_train)\n",
    "tree_grid_search_7 = GridSearchCV(DecisionTreeClassifier(), tree_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(Tree_X_train_7, Y_train)\n",
    "tree_grid_search_8 = GridSearchCV(DecisionTreeClassifier(), tree_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(Tree_X_train_8, Y_train)\n",
    "\n",
    "# Extracting the best model\n",
    "Tree_model_6 = tree_grid_search_6.best_estimator_\n",
    "Tree_score_6 = tree_grid_search_6.cv_results_\n",
    "Tree_score_6 = Tree_score_6['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Decision Tree Classifier with the top 6 features\")\n",
    "print(\"Model: \", Tree_model_6)\n",
    "print(\"Score: \", round(Tree_score_6, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Extracting the best model\n",
    "Tree_model_7 = tree_grid_search_7.best_estimator_\n",
    "Tree_score_7 = tree_grid_search_7.cv_results_\n",
    "Tree_score_7 = Tree_score_7['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Decision Tree Classifier with the top 7 features\")\n",
    "print(\"Model: \", Tree_model_7)\n",
    "print(\"Score: \", round(Tree_score_7, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Extracting the best model\n",
    "Tree_model_8 = tree_grid_search_8.best_estimator_\n",
    "Tree_score_8 = tree_grid_search_8.cv_results_\n",
    "Tree_score_8 = Tree_score_8['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Decision Tree Classifier with the top 8 features\")\n",
    "print(\"Model: \", Tree_model_8)\n",
    "print(\"Score: \", round(Tree_score_8, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "\n",
    "# Defining top 8, 7, and 6 variables\n",
    "# Train dataset Gradient Boosting Classifier\n",
    "GBC_X_train_8 = X_train[['Tree_1', 'Log_BMI', 'Interaction_5', 'Interaction_3', 'Interaction_1', 'GenHlth', 'Age', '75,000+']]\n",
    "GBC_X_train_7 = X_train[['Tree_1', 'Log_BMI', 'Interaction_5', 'Interaction_3', 'Interaction_1', 'GenHlth', 'Age']]\n",
    "GBC_X_train_6 = X_train[['Tree_1', 'Log_BMI', 'Interaction_5', 'Interaction_3', 'Interaction_1', 'GenHlth']]\n",
    "\n",
    "\n",
    "## Defining the hyper-parameters for Gradient Boosting Classifier\n",
    "GBC_param_grid =  {'n_estimators': [100, 300, 500],\n",
    "                  'min_samples_split': [10, 15],\n",
    "                  'min_samples_leaf': [5, 7],\n",
    "                  'max_depth': [3, 5, 7],\n",
    "                  'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "# Performing GridSearch\n",
    "GBC_grid_search_6 = GridSearchCV(GradientBoostingClassifier(), GBC_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(GBC_X_train_6, Y_train)\n",
    "GBC_grid_search_7 = GridSearchCV(GradientBoostingClassifier(), GBC_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(GBC_X_train_7, Y_train)\n",
    "GBC_grid_search_8 = GridSearchCV(GradientBoostingClassifier(), GBC_param_grid, cv = 3, scoring = 'f1_micro', n_jobs = -1).fit(GBC_X_train_8, Y_train)\n",
    "\n",
    "# Extracting the best model\n",
    "GBC_model_6 = GBC_grid_search_6.best_estimator_\n",
    "GBC_score_6 = GBC_grid_search_6.cv_results_\n",
    "GBC_score_6 = GBC_score_6['mean_test_score'][0]\n",
    "\n",
    "\n",
    "# Printing the results\n",
    "print(\"Gradient Boosting Classifier with the top 6 features\")\n",
    "print(\"Model: \", GBC_model_6)\n",
    "print(\"Score: \", round(GBC_score_6, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "# Extracting the best model\n",
    "GBC_model_7 = GBC_grid_search_7.best_estimator_\n",
    "GBC_score_7 = GBC_grid_search_7.cv_results_\n",
    "GBC_score_7 = GBC_score_7['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Gradient Boosting Classifier with the top 7 features\")\n",
    "print(\"Model: \", GBC_model_7)\n",
    "print(\"Score: \", round(GBC_score_7, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "# Extracting the best model\n",
    "GBC_model_8 = GBC_grid_search_8.best_estimator_\n",
    "GBC_score_8 = GBC_grid_search_8.cv_results_\n",
    "GBC_score_8 = GBC_score_8['mean_test_score'][0]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Gradient Boosting Classifier with the top 8 features\")\n",
    "print(\"Model: \", GBC_model_8)\n",
    "print(\"Score: \", round(GBC_score_8, 3))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "# List of models\n",
    "models = [RF_model_6, RF_model_7, RF_model_8,\n",
    "         Ada_model_6, Ada_model_7, Ada_model_8,\n",
    "         Tree_model_6, Tree_model_7, Tree_model_8,\n",
    "         GBC_model_6, GBC_model_8, GBC_model_8]\n",
    "\n",
    "# List of scores\n",
    "scores = [RF_score_6, RF_score_7, RF_score_8,\n",
    "         Ada_score_6, Ada_score_7, Ada_score_8,\n",
    "         Tree_score_6, Tree_score_7, Tree_score_8,\n",
    "         GBC_score_6, GBC_score_7, GBC_score_8]\n",
    "\n",
    "# Number of top features used\n",
    "variables = [\"top 6\", \"top 7\", \"top 8\",\n",
    "            \"top 6\", \"top 7\", \"top 8\",\n",
    "            \"top 6\", \"top 7\", \"top 8\",\n",
    "            \"top 6\", \"top 7\", \"top 8\"]\n",
    "\n",
    "# Saving results in a data-frame\n",
    "results = pd.DataFrame({\"Models\": models, \"Variables\": variables, \"Scores\": scores})\n",
    "\n",
    "# Exporting as csv\n",
    "results.to_csv(\"best_models.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
